# mean_ = torch.FloatTensor([0.485, 0.456, 0.406]).to(device)
# std_ = torch.FloatTensor([0.229, 0.224, 0.225]).to(device)


# mean_13= torch.FloatTensor([58.77515069,27.25982541,-7.825135523]).to(device)
# std_13 =torch.FloatTensor([20.70605765,12.67681833,6.666722412]).to(device)

# mean_2= torch.FloatTensor([50.69584084,33.70910395,-9.417649872]).to(device)
# std_2= torch.FloatTensor([14.18854242,7.071403321,6.476873549]).to(device)

# mean_6= torch.FloatTensor([26.38038809,21.939772,-18.54567921]).to(device)
# std_6= torch.FloatTensor([21.07028115,6.905576127,8.149537741]).to(device)

# mean_8= torch.FloatTensor([52.40771309,15.71980628,-14.10899633]).to(device)
# std_8= torch.FloatTensor([30.20962623,9.022655809,11.06458503]).to(device)

# mean_9= torch.FloatTensor([69.79752839,18.39389349,-8.408362564]).to(device)
# std_9 =torch.FloatTensor([19.91183487,11.46137616,7.233493937]).to(device)


# mean_lab = torch.FloatTensor([51.61 ,23.40 ,-11.66]).to(device)
# std_lab = torch.FloatTensor([21.22 ,9.43,7.92]).to(device)


# mean_hsv = torch.FloatTensor([0.82702367,0.37666315,0.631354464]).to(device)
# std_hsv = torch.FloatTensor([0.147702595,0.171121216,0.19210163]).to(device)

# mean_hed = torch.FloatTensor([0.6411, -0.3809, 0.5712]).to(device)
# std_hed = torch.FloatTensor([0.3239, 0.0990, 0.1606]).to(device)

# gpu_id: 1
seed: 42
### Roots
train_root: "/root/autodl-tmp/RandStainNA-master/classification/100K-NONORM-png/train0"
# train_root: "/home/yiqing/data/crc/NCT-CRC-HE-100K-NONORM-8class"
# train_root: "/home/yiqing/data/crc/NCT-CRC-HE-100K-NONORM"
# train_root: "/home/yiqing/data/crc/NCT-CRC-HE-100K"
# train_root: "/home/yiqing/data/crc/NCT-CRC-HE-100K-NONORM-LAB-NORM-V3"
# test_root: "/home/yiqing/data/crc/CRC-VAL-HE-7K"
# test_root: "/home/yiqing/data/crc/CRC-VAL-HE-7K-8class"
test_root: "/root/autodl-tmp/RandStainNA-master/classification/100K-NONORM-png/test"
output_path: "/root/autodl-tmp/staintrick/results/" 
postfix: 'NONORM8class_hed'
### Model & Schemes
model: "resnet18" # model_name in Timm
pretrained: False # If to use the ImageNet pretrained weights
prenorm: False # If to use the prenorm scheme
emaprenorm: False # If to use the EMA variant of prenorm scheme
emaprenorm_lambda: 0.01
randnorm: False # If to use the randnorm scheme
temnorm: False 
hsvnorm: False 
hednorm: True 
### Training Configurations
num_workers: 15
batch_size: 256
learning_rate: 5.0e-4
weight_decay: 1.0e-5
min_learning_rate: 5.0e-6
T_max: 10
gamma: 0.8
epochs: 100
patience: 8
scheduler: 'cosine' 
mu0: [0.6411, -0.3809, 0.5712]
sigma0: [0.3239, 0.0990, 0.1606]